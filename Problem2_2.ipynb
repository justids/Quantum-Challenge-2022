{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import time\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import pennylane as qml\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Costum dataset 생성\n",
    "class FacadeDataset(Dataset):\n",
    "    def __init__(self, path2img, direction='b2a', transform=False):\n",
    "        super().__init__()\n",
    "        self.direction = direction\n",
    "        self.path2a = join(path2img, 'a')\n",
    "        self.path2b = join(path2img, 'b')\n",
    "        self.img_filenames = [x for x in listdir(self.path2a)]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        a = Image.open(join(self.path2a, self.img_filenames[index])).convert('RGB')\n",
    "        b = Image.open(join(self.path2b, self.img_filenames[index])).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            a = self.transform(a)\n",
    "            b = self.transform(b)\n",
    "        a=(a+1)/2\n",
    "        b=(b+1)/2\n",
    "        \n",
    "\n",
    "        if self.direction == 'b2a':\n",
    "            return b,a\n",
    "        else:\n",
    "            return a,b\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5]),\n",
    "                    transforms.Resize((256,256))\n",
    "])\n",
    "path2img = 'data/facades/train'\n",
    "train_ds = FacadeDataset(path2img, transform=transform)\n",
    "train_dl = DataLoader(train_ds, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNet\n",
    "class UNetDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, normalize=True, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, 4, stride=2, padding=1, bias=False)]\n",
    "\n",
    "        if normalize:\n",
    "            layers.append(nn.InstanceNorm2d(out_channels)),\n",
    "\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        self.down = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.down(x)\n",
    "        return x\n",
    "    \n",
    "class UNetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels,4,2,1,bias=False),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU()\n",
    "        ]\n",
    "\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        self.up = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "    \n",
    "n_qubits = 6\n",
    "dev = qml.device(\"lightning.qubit\", wires=n_qubits)\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def qc(inputs, weights):\n",
    "    qml.AmplitudeEmbedding(features=inputs, wires=range(n_qubits),normalize=True)\n",
    "    q_depth=3\n",
    "    k=0\n",
    "\n",
    "\n",
    "    # Repeated layer\n",
    "    for i in range(q_depth):\n",
    "        # Parameterised layer\n",
    "        for y in range(n_qubits):\n",
    "            qml.RY(weights[k], wires=y)\n",
    "            k+=1\n",
    "\n",
    "        # Control Z gates\n",
    "        for y in range(n_qubits - 1):\n",
    "            qml.CZ(wires=[y, y + 1])\n",
    "\n",
    "    return qml.probs(wires=list(range(n_qubits)))\n",
    "\n",
    "weight_shapes = {\"weights\": n_qubits*3}\n",
    "qlayer = qml.qnn.TorchLayer(qc, weight_shapes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 3.1692e-01,  2.1175e+00, -4.4934e-01,  ...,  2.1827e-01,\n",
      "           -4.1943e-01, -7.7055e-01],\n",
      "          [-2.4606e-01, -5.6883e-01,  7.5542e-02,  ...,  6.3938e-01,\n",
      "            1.0312e-01,  7.6918e-01],\n",
      "          [-1.9574e-01, -3.0436e-01,  1.2157e-01,  ...,  2.7604e-01,\n",
      "           -3.4550e-02,  1.0803e-01],\n",
      "          ...,\n",
      "          [-6.4386e-01,  3.1231e-01, -7.5478e-01,  ..., -2.1602e-03,\n",
      "           -5.3222e-01, -8.1635e-02],\n",
      "          [ 7.3150e-01,  9.0138e-03,  2.7234e-01,  ...,  1.8248e-01,\n",
      "            9.5824e-01,  3.3488e-02],\n",
      "          [-8.1353e-01,  3.4577e-01, -8.1151e-02,  ..., -6.1934e-01,\n",
      "           -9.4608e-01,  1.9028e+00]],\n",
      "\n",
      "         [[-9.4013e-01, -1.3689e+00, -9.2546e-01,  ..., -1.3012e+00,\n",
      "            2.8168e-01, -1.4023e+00],\n",
      "          [ 4.2357e-02, -1.1090e+00,  4.6232e-01,  ..., -1.4432e+00,\n",
      "            1.2009e+00, -2.9127e-01],\n",
      "          [ 1.5367e-01,  5.8763e-01,  4.5354e-01,  ..., -9.0218e-02,\n",
      "            1.7929e-01, -1.8051e+00],\n",
      "          ...,\n",
      "          [-2.0466e+00,  5.8618e-01, -6.9491e-01,  ..., -1.4292e+00,\n",
      "            1.6074e-02, -1.1170e+00],\n",
      "          [-9.7443e-02, -5.2362e-01, -9.8795e-01,  ...,  1.2254e+00,\n",
      "            1.7014e+00,  2.3118e+00],\n",
      "          [ 2.9843e-02,  1.3233e-01, -1.5533e+00,  ...,  7.2739e-02,\n",
      "           -8.0057e-01, -1.1891e+00]],\n",
      "\n",
      "         [[-1.5891e+00, -1.1886e+00, -7.7469e-02,  ..., -2.9152e-01,\n",
      "            6.0716e-01,  1.7627e+00],\n",
      "          [ 1.9913e-01,  4.7298e-01,  1.2381e+00,  ..., -8.4402e-01,\n",
      "           -2.4167e-01, -3.3962e-01],\n",
      "          [-7.6479e-01, -1.0202e-01,  1.4840e+00,  ..., -9.4868e-01,\n",
      "           -1.9661e-01,  2.7823e-01],\n",
      "          ...,\n",
      "          [-1.9923e-01, -1.3749e-01,  8.3468e-01,  ...,  4.8614e-01,\n",
      "           -4.7531e-01,  1.2200e-01],\n",
      "          [ 7.4834e-02, -7.7313e-01, -1.2187e+00,  ..., -4.6476e-01,\n",
      "           -2.3250e-01,  2.5969e+00],\n",
      "          [ 3.4851e-01, -1.9780e+00,  6.1270e-01,  ..., -2.9372e+00,\n",
      "           -2.6474e-01, -1.4622e+00]]]], device='cuda:0')\n",
      "torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "class GeneratorUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.down1 = UNetDown(in_channels, 8, normalize=False)\n",
    "        self.down2 = UNetDown(8,16)                 \n",
    "        self.down3 = UNetDown(16,32)               \n",
    "        self.down4 = UNetDown(32,64,dropout=0.5) \n",
    "        self.down5 = UNetDown(64,64,dropout=0.5)      \n",
    "        self.down6 = UNetDown(64,64,dropout=0.5)             \n",
    "        self.down7 = UNetDown(64,64,dropout=0.5)              \n",
    "        self.down8 = UNetDown(64,64,normalize=False,dropout=0.5)\n",
    "        self.qlayer = qlayer\n",
    "        self.up1 = UNetUp(64,64,dropout=0.5)\n",
    "        self.up2 = UNetUp(64,64,dropout=0.5)\n",
    "        self.up3 = UNetUp(64,64,dropout=0.5)\n",
    "        self.up4 = UNetUp(64,64,dropout=0.5)\n",
    "        self.up5 = UNetUp(64,32)\n",
    "        self.up6 = UNetUp(32,16)\n",
    "        self.up7 = UNetUp(16,8)\n",
    "        self.up8 = nn.Sequential(\n",
    "            UNetUp(8,3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(x)\n",
    "        d1 = self.down1(x)\n",
    "        \n",
    "        d2 = self.down2(d1)\n",
    "       \n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        d8 = self.down8(d7)\n",
    "        d8=d8.reshape(64)\n",
    "        amp=d8.norm()\n",
    "        d8=d8/amp\n",
    "        c0=self.qlayer(d8)*amp\n",
    "        c0=c0.reshape(1,64,1,1)\n",
    "        u1 = self.up1(c0)\n",
    "        u2 = self.up2(u1)\n",
    "        u3 = self.up3(u2)\n",
    "        u4 = self.up4(u3)\n",
    "        u5 = self.up5(u4)\n",
    "        u6 = self.up6(u5)\n",
    "        u7 = self.up7(u6)\n",
    "        u8 = self.up8(u7)\n",
    "        return u8\n",
    "\n",
    "# check\n",
    "x = torch.randn(1,3,256,256,device=device)\n",
    "model = GeneratorUNet().to(device)\n",
    "out = model(x)\n",
    "print(out.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 128, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "class Dis_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, normalize=True):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, 3, stride=2, padding=1)]\n",
    "        if normalize:\n",
    "            layers.append(nn.InstanceNorm2d(out_channels))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "    \n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "        return x\n",
    "\n",
    "# check\n",
    "x = torch.randn(16,64,128,128,device=device)\n",
    "model = Dis_block(64,128).to(device)\n",
    "out = model(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.stage_1 = Dis_block(in_channels*2,64,normalize=False)\n",
    "        self.stage_2 = Dis_block(64,128)\n",
    "        self.stage_3 = Dis_block(128,256)\n",
    "        self.stage_4 = Dis_block(256,512)\n",
    "\n",
    "        self.patch = nn.Conv2d(512,1,3,padding=1) # 16x16 패치 생성\n",
    "\n",
    "    def forward(self,a,b):\n",
    "        x = torch.cat((a,b),1)\n",
    "        x = self.stage_1(x)\n",
    "        x = self.stage_2(x)\n",
    "        x = self.stage_3(x)\n",
    "        x = self.stage_4(x)\n",
    "        x = self.patch(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "# check\n",
    "x = torch.randn(16,3,256,256,device=device)\n",
    "model = Discriminator().to(device)\n",
    "out = model(x,x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    class_name = model.__class__.__name__\n",
    "    if class_name.find('Conv') != -1:\n",
    "        nn.init.normal_(model.weight.data, 0.0, 0.02)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func_gan = nn.BCELoss()\n",
    "loss_func_pix = nn.L1Loss()\n",
    "\n",
    "# loss_func_pix 가중치\n",
    "lambda_pixel = 100\n",
    "\n",
    "# patch 수\n",
    "patch = (1,256//2**4,256//2**4)\n",
    "\n",
    "# 최적화 파라미터\n",
    "from torch import optim\n",
    "lr = 2e-4\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "model_dis=Discriminator().cuda()\n",
    "model_gen=GeneratorUNet().cuda()\n",
    "opt_dis = optim.Adam(model_dis.parameters(),lr=lr,betas=(beta1,beta2))\n",
    "opt_gen = optim.Adam(model_gen.parameters(),lr=lr,betas=(beta1,beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.6667, 0.6667, 0.6667,  ..., 0.6667, 0.6667, 0.6667],\n",
      "          [0.6667, 0.6667, 0.6667,  ..., 0.6667, 0.6667, 0.6667],\n",
      "          [0.6667, 0.6667, 0.6667,  ..., 0.6667, 0.6667, 0.6667],\n",
      "          ...,\n",
      "          [0.6667, 0.6667, 0.6667,  ..., 0.6667, 0.6667, 0.6667],\n",
      "          [0.6667, 0.6667, 0.6667,  ..., 0.6667, 0.6667, 0.6667],\n",
      "          [0.6667, 0.6667, 0.6667,  ..., 0.6667, 0.6667, 0.6667]]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.6667, 0.6667, 0.6667,  ..., 0.6667, 0.6667, 0.6667],\n",
      "          [0.6667, 0.6667, 0.6667,  ..., 0.6667, 0.6667, 0.6667],\n",
      "          [0.6667, 0.6667, 0.6667,  ..., 0.6667, 0.6667, 0.6667],\n",
      "          ...,\n",
      "          [0.6667, 0.6667, 0.6667,  ..., 0.6667, 0.6667, 0.6667],\n",
      "          [0.6667, 0.6667, 0.6667,  ..., 0.6667, 0.6667, 0.6667],\n",
      "          [0.6667, 0.6667, 0.6667,  ..., 0.6667, 0.6667, 0.6667]]]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W Copy.cpp:244] Warning: Casting complex values to real discards the imaginary part (function operator())\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "State vectors have to be of norm 1.0, vector 0 has norm nan",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10719/2993026072.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mmodel_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mfake_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_a\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 가짜 이미지 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mout_dis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_dis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_b\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 가짜 이미지 식별\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_10719/2805833801.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mamp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md8\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0md8\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md8\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mc0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mc0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mu1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.7/site-packages/pennylane/qnn/torch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;31m# If the input is 1-dimensional, calculate the forward pass as usual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate_qnode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_evaluate_qnode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.7/site-packages/pennylane/qnn/torch.py\u001b[0m in \u001b[0;36m_evaluate_qnode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    329\u001b[0m         }\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     def _init_weights(\n",
      "\u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.7/site-packages/pennylane/qnode.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         \u001b[0;31m# construct the tape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cache\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.7/site-packages/pennylane/qnode.py\u001b[0m in \u001b[0;36mconstruct\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;31m# all operations are supported by the transform.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.7/site-packages/pennylane/transforms/tape_expand.py\u001b[0m in \u001b[0;36mexpand_fn\u001b[0;34m(tape, depth, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mtape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_at\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mtape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop_at\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.7/site-packages/pennylane/tape/tape.py\u001b[0m in \u001b[0;36mexpand\u001b[0;34m(self, depth, stop_at, expand_measurements)\u001b[0m\n\u001b[1;32m    609\u001b[0m         \"\"\"\n\u001b[1;32m    610\u001b[0m         new_tape = expand_tape(\n\u001b[0;32m--> 611\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop_at\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_measurements\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpand_measurements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m         )\n\u001b[1;32m    613\u001b[0m         \u001b[0mnew_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.7/site-packages/pennylane/tape/tape.py\u001b[0m in \u001b[0;36mexpand_tape\u001b[0;34m(tape, depth, stop_at, expand_measurements)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0;31m# recursively expand out the newly created tape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mexpanded_tape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpand_tape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop_at\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mnew_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mexpanded_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.7/site-packages/pennylane/tape/tape.py\u001b[0m in \u001b[0;36mexpand_tape\u001b[0;34m(tape, depth, stop_at, expand_measurements)\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;31m# Object is an operation; query it for its expansion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m                     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mDecompositionUndefinedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                     \u001b[0;31m# Object does not define an expansion; treat this as\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.7/site-packages/pennylane/operation.py\u001b[0m in \u001b[0;36mexpand\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.7/site-packages/pennylane/operation.py\u001b[0m in \u001b[0;36mdecomposition\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \"\"\"\n\u001b[1;32m   1026\u001b[0m         return self.compute_decomposition(\n\u001b[0;32m-> 1027\u001b[0;31m             \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwires\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwires\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m         )\n\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.7/site-packages/pennylane/ops/qubit/state_preparation.py\u001b[0m in \u001b[0;36mcompute_decomposition\u001b[0;34m(state, wires)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \"\"\"\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mMottonenStatePreparation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwires\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.7/site-packages/pennylane/templates/state_preparations/mottonen.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, state_vector, wires, do_queue, id)\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mqml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 raise ValueError(\n\u001b[0;32m--> 315\u001b[0;31m                     \u001b[0;34mf\"State vectors have to be of norm 1.0, vector {i} has norm {norm}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m                 )\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: State vectors have to be of norm 1.0, vector 0 has norm nan"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "model_gen.train()\n",
    "model_dis.train()\n",
    "\n",
    "batch_count = 0\n",
    "num_epochs = 20\n",
    "start_time = time.time()\n",
    "\n",
    "loss_hist = {'gen':[],\n",
    "             'dis':[]}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for a, b in train_dl:\n",
    "        ba_si = a.size(0)\n",
    "\n",
    "        # real image\n",
    "        real_a = a.to(device)\n",
    "        real_b = b.to(device)\n",
    "\n",
    "        # patch label\n",
    "        real_label = torch.ones(ba_si, *patch, requires_grad=False).to(device)\n",
    "        fake_label = torch.zeros(ba_si, *patch, requires_grad=False).to(device)\n",
    "\n",
    "        # generator\n",
    "        model_gen.zero_grad()\n",
    "\n",
    "        fake_b = model_gen(real_a) # 가짜 이미지 생성\n",
    "        out_dis = model_dis(fake_b, real_b) # 가짜 이미지 식별\n",
    "        \n",
    "\n",
    "        gen_loss = loss_func_gan(out_dis, real_label)\n",
    "        pixel_loss = loss_func_pix(fake_b, real_b)\n",
    "\n",
    "        g_loss = gen_loss + lambda_pixel * pixel_loss\n",
    "        g_loss.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        # discriminator\n",
    "        model_dis.zero_grad()\n",
    "\n",
    "        out_dis = model_dis(real_b, real_a) # 진짜 이미지 식별\n",
    "        real_loss = loss_func_gan(out_dis,real_label)\n",
    "        \n",
    "        out_dis = model_dis(fake_b.detach(), real_a) # 가짜 이미지 식별\n",
    "        fake_loss = loss_func_gan(out_dis,fake_label)\n",
    "\n",
    "        d_loss = (real_loss + fake_loss) / 2.\n",
    "        d_loss.backward()\n",
    "        opt_dis.step()\n",
    "\n",
    "        loss_hist['gen'].append(g_loss.item())\n",
    "        loss_hist['dis'].append(d_loss.item())\n",
    "\n",
    "        batch_count += 1\n",
    "        if batch_count % 10 == 0:\n",
    "            print('Epoch: %.0f, G_Loss: %.6f, D_Loss: %.6f, time: %.2f min' %(epoch, g_loss.item(), d_loss.item(), (time.time()-start_time)/60) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('qiskit': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c17a849593e93bae2879273e9f8c9f0810d0d90f15e79b07e288458ad6002409"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
